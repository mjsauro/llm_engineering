{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Website Scraper - Usage Examples\n",
    "\n",
    "This notebook demonstrates how to use the `AdvancedWebsite` class for web scraping.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Handles JavaScript-rendered pages** (React, Vue, etc.) using Playwright\n",
    "- **Automatic fallback** to BeautifulSoup for static sites (faster)\n",
    "- **Better error handling** with retry logic\n",
    "- **Enhanced text extraction** with improved cleaning\n",
    "- **Metadata extraction** (description, keywords, Open Graph tags)\n",
    "- **Improved link extraction** with validation and normalization\n",
    "- **Built-in summarization** using Ollama via OpenAI-compatible API\n",
    "\n",
    "## Important Note\n",
    "\n",
    "If you update `advanced_website_scraper.py` and the changes don't appear, **re-run Cell 2** (the import cell) to reload the module. The import cell includes automatic reloading, but you may need to re-run it after making changes to the Python file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "If you want to use Playwright for JavaScript-rendered pages, you'll need to install it:\n",
    "\n",
    "```bash\n",
    "pip install playwright\n",
    "playwright install\n",
    "```\n",
    "\n",
    "Note: Playwright is optional. The scraper will fall back to requests + BeautifulSoup if Playwright is not available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the AdvancedWebsite class\n",
    "# Note: This assumes advanced_website_scraper.py is in the same directory\n",
    "# If running from a different location, you may need to adjust the import path\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "# Add current directory to path if needed\n",
    "current_dir = os.path.dirname(os.path.abspath(''))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "# Import the module\n",
    "import advanced_website_scraper\n",
    "\n",
    "# Reload the module to pick up any changes (useful during development)\n",
    "importlib.reload(advanced_website_scraper)\n",
    "\n",
    "# Import the class\n",
    "from advanced_website_scraper import AdvancedWebsite\n",
    "\n",
    "print(\"✓ AdvancedWebsite class imported successfully\")\n",
    "print(f\"✓ Available methods: {[m for m in dir(AdvancedWebsite) if not m.startswith('_')]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your website URL and model name here. Change these variables to test different websites and models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables - Change these to test different websites and models\n",
    "WEBSITE_URL = \"https://streamlit.io\"  # Change this to any website URL\n",
    "MODEL_NAME = \"llama3.1\"  # Change this to your preferred Ollama model (e.g., \"llama3.2\", \"llama3.1\", etc.)\n",
    "USE_JS = True  # Set to True to force JavaScript rendering (requires Playwright)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Website URL: {WEBSITE_URL}\")\n",
    "print(f\"  Model Name: {MODEL_NAME}\")\n",
    "print(f\"  Use JavaScript: {USE_JS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Scraping a Static Website (Fast)\n",
    "\n",
    "For static websites, the scraper will use requests + BeautifulSoup, which is faster and lighter.\n",
    "\n",
    "**Note:** This example uses the `WEBSITE_URL` variable defined in the configuration cell above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a static website using the configured URL\n",
    "website = AdvancedWebsite(WEBSITE_URL, use_js=USE_JS)\n",
    "\n",
    "print(f\"Fetch method: {website.get_fetch_method()}\")\n",
    "print(f\"\\nTitle: {website.title}\")\n",
    "print(f\"\\nContent preview (first 500 chars):\\n{website.text[:500]}\")\n",
    "print(f\"\\nNumber of links found: {len(website.get_links())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Scraping a JavaScript-Rendered Website\n",
    "\n",
    "For websites that use JavaScript to render content (like React, Vue, etc.), use Playwright mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a JavaScript-rendered website (like OpenAI)\n",
    "# Note: This requires Playwright to be installed\n",
    "try:\n",
    "    website = AdvancedWebsite(\"https://openai.com\", use_js=True, timeout=60)\n",
    "    \n",
    "    print(f\"Fetch method: {website.get_fetch_method()}\")\n",
    "    print(f\"\\nTitle: {website.title}\")\n",
    "    print(f\"\\nContent preview (first 500 chars):\\n{website.text[:500]}\")\n",
    "    print(f\"\\nNumber of links found: {len(website.get_links())}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure Playwright is installed: pip install playwright && playwright install\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Automatic Mode (Smart Fallback)\n",
    "\n",
    "The scraper can automatically detect the best method. It tries requests first (faster), and falls back to Playwright if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let the scraper decide the best method using the configured URL\n",
    "website = AdvancedWebsite(WEBSITE_URL)\n",
    "\n",
    "print(f\"Fetch method used: {website.get_fetch_method()}\")\n",
    "print(f\"\\nTitle: {website.title}\")\n",
    "print(f\"\\nContent preview (first 500 chars):\\n{website.text[:500]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Getting Formatted Contents\n",
    "\n",
    "Use the `get_contents()` method to get a formatted string with title and content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = AdvancedWebsite(WEBSITE_URL)\n",
    "\n",
    "# Get full contents\n",
    "contents = website.get_contents()\n",
    "print(contents[:1000])  # Print first 1000 characters\n",
    "\n",
    "# Get contents with length limit\n",
    "limited_contents = website.get_contents(max_length=500)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Limited to 500 characters:\")\n",
    "print(limited_contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Extracting Links\n",
    "\n",
    "Get all links found on the page, with automatic validation and normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = AdvancedWebsite(\"https://techcrunch.com/\")\n",
    "\n",
    "links = website.get_links()\n",
    "print(f\"Found {len(links)} links:\")\n",
    "for i, link in enumerate(links[:10], 1):  # Show first 10 links\n",
    "    print(f\"{i}. {link}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Extracting Metadata\n",
    "\n",
    "Extract metadata like description, keywords, Open Graph tags, and Twitter Card tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = AdvancedWebsite(\"https://openai.com\")\n",
    "\n",
    "metadata = website.get_metadata()\n",
    "print(\"Extracted Metadata:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in metadata.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: Waiting for Dynamic Content\n",
    "\n",
    "If a page loads content dynamically, you can wait for a specific selector before extracting content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Wait for a specific element to load\n",
    "# This is useful for pages that load content via JavaScript\n",
    "# website = AdvancedWebsite(\n",
    "#     WEBSITE_URL,\n",
    "#     use_js=True,\n",
    "#     wait_for_selector=\"main-content\"  # Wait for element with this ID or class\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two approaches\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Basic approach (from the original scraper)\n",
    "def basic_scrape(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    title = soup.title.string if soup.title else \"No title found\"\n",
    "    if soup.body:\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "    else:\n",
    "        text = \"\"\n",
    "    return title, text\n",
    "\n",
    "# Advanced approach\n",
    "url = \"https://openai.com\"\n",
    "print(\"Basic scraper:\")\n",
    "basic_title, basic_text = basic_scrape(url)\n",
    "print(f\"Title: {basic_title}\")\n",
    "print(f\"Text length: {len(basic_text)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Advanced scraper:\")\n",
    "advanced = AdvancedWebsite(url, use_js=False)\n",
    "print(f\"Title: {advanced.title}\")\n",
    "print(f\"Text length: {len(advanced.text)}\")\n",
    "print(f\"Links found: {len(advanced.get_links())}\")\n",
    "print(f\"Metadata keys: {list(advanced.get_metadata().keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website Summarization with Ollama\n",
    "\n",
    "The `AdvancedWebsite` class includes built-in summarization using Ollama via the OpenAI-compatible API. This allows you to automatically summarize any scraped website using local LLM models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites for Summarization\n",
    "\n",
    "Before using summarization, make sure:\n",
    "1. Ollama is installed and running: `ollama serve`\n",
    "2. A model is available: `ollama pull llama3.1` (or llama3.2, etc.)\n",
    "3. OpenAI library is installed: `pip install openai`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 8: Basic Website Summarization\n",
    "\n",
    "Scrape a website and summarize it using Ollama.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape and summarize a website\n",
    "# Note: Some sites like openai.com have bot protection (403 errors)\n",
    "# Use example.com or other sites for testing\n",
    "\n",
    "try:\n",
    "    # Use a simple site for testing (openai.com has bot protection)\n",
    "    website = AdvancedWebsite(\"https://example.com\", use_js=False)\n",
    "    \n",
    "    print(\"Website Title:\", website.title)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Original Content (first 300 chars):\")\n",
    "    print(website.text[:300])\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Summary using Ollama:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check if summarize method exists\n",
    "    if hasattr(website, 'summarize'):\n",
    "        # Summarize using default settings\n",
    "        summary = website.summarize(model=\"llama3.1\", temperature=0)\n",
    "        print(summary)\n",
    "    else:\n",
    "        print(\"ERROR: summarize method not found!\")\n",
    "        print(\"\\nSOLUTION: Please re-run the import cell (Cell 2) above to reload the module.\")\n",
    "        print(\"Or restart the kernel: Kernel -> Restart Kernel\")\n",
    "        print(f\"\\nAvailable methods: {[m for m in dir(website) if not m.startswith('_')]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    error_msg = str(e)\n",
    "    print(f\"Error: {error_msg}\")\n",
    "    \n",
    "    if \"'AdvancedWebsite' object has no attribute 'summarize'\" in error_msg:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SOLUTION: The module needs to be reloaded!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"1. Go back to Cell 2 (the import cell)\")\n",
    "        print(\"2. Re-run that cell (Shift+Enter)\")\n",
    "        print(\"3. Then come back and run this cell again\")\n",
    "        print(\"\\nOR restart the kernel:\")\n",
    "        print(\"   Kernel -> Restart Kernel -> Restart\")\n",
    "    else:\n",
    "        print(\"\\nTroubleshooting:\")\n",
    "        print(\"1. Make sure Ollama is running: ollama serve\")\n",
    "        print(\"2. Make sure model is available: ollama pull llama3.1\")\n",
    "        print(\"3. Make sure OpenAI library is installed: pip install openai\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 9: Advanced Summarization with Custom Settings\n",
    "\n",
    "Use custom prompts, temperature, and other parameters for summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    website = AdvancedWebsite(WEBSITE_URL, use_js=USE_JS)\n",
    "    \n",
    "    # Custom system prompt for more detailed summary\n",
    "    custom_prompt = (\n",
    "        \"You are an expert content analyst. Provide a comprehensive summary \"\n",
    "        \"that includes: 1) Main topic, 2) Key points, 3) Important details, \"\n",
    "        \"4) Any actionable insights. Format your response clearly.\"\n",
    "    )\n",
    "    \n",
    "    summary = website.summarize_with_ollama(\n",
    "        model=MODEL_NAME,\n",
    "        temperature=0.3,  # Slightly more creative\n",
    "        max_tokens=500,   # Limit response length\n",
    "        system_prompt=custom_prompt\n",
    "    )\n",
    "    \n",
    "    print(\"Detailed Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(summary)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 10: Summarizing JavaScript-Rendered Websites\n",
    "\n",
    "Combine advanced scraping with summarization for JavaScript-heavy sites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape a JavaScript-rendered site and summarize it\n",
    "try:\n",
    "    # Note: This requires Playwright to be installed\n",
    "    website = AdvancedWebsite(WEBSITE_URL, use_js=USE_JS)\n",
    "    \n",
    "    print(f\"Scraped using: {website.get_fetch_method()}\")\n",
    "    print(f\"Title: {website.title}\")\n",
    "    print(f\"Content length: {len(website.text)} characters\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    summary = website.summarize(model=\"llama3.1\", temperature=0)\n",
    "    print(summary)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nIf you see Playwright errors, you can:\")\n",
    "    print(\"1. Install Playwright: pip install playwright && playwright install\")\n",
    "    print(\"2. Or use use_js=False for static sites\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 11: Complete Workflow - Scrape, Extract, and Summarize\n",
    "\n",
    "A complete example showing the full workflow from scraping to summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_summarize(url, model, use_js=False):\n",
    "    \"\"\"\n",
    "    Complete workflow: Scrape a website and summarize it.\n",
    "    \n",
    "    Args:\n",
    "        url: URL to scrape\n",
    "        model: Ollama model to use for summarization\n",
    "        use_js: Whether to use JavaScript rendering\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with scraped data and summary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Scrape the website\n",
    "        print(f\"Scraping {url}...\")\n",
    "        website = AdvancedWebsite(url, use_js=use_js)\n",
    "        print(f\"✓ Scraped using: {website.get_fetch_method()}\")\n",
    "        \n",
    "        # Step 2: Extract information\n",
    "        print(f\"✓ Title: {website.title}\")\n",
    "        print(f\"✓ Content length: {len(website.text)} characters\")\n",
    "        print(f\"✓ Links found: {len(website.get_links())}\")\n",
    "        print(f\"✓ Metadata keys: {list(website.get_metadata().keys())}\")\n",
    "        \n",
    "        # Step 3: Summarize\n",
    "        print(f\"\\nGenerating summary using {model}...\")\n",
    "        summary = website.summarize(model=model, temperature=0)\n",
    "        print(\"✓ Summary generated\")\n",
    "        \n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"title\": website.title,\n",
    "            \"content_length\": len(website.text),\n",
    "            \"links_count\": len(website.get_links()),\n",
    "            \"metadata\": website.get_metadata(),\n",
    "            \"summary\": summary,\n",
    "            \"fetch_method\": website.get_fetch_method()\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage with configured variables\n",
    "result = scrape_and_summarize(WEBSITE_URL, model=MODEL_NAME, use_js=USE_JS)\n",
    "\n",
    "if result:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY:\")\n",
    "    print(\"=\"*50)\n",
    "    print(result[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `AdvancedWebsite` class now provides a complete solution for web scraping and summarization:\n",
    "\n",
    "1. **Advanced Scraping**: Handles both static and JavaScript-rendered pages\n",
    "2. **Smart Fallback**: Automatically chooses the best method\n",
    "3. **Enhanced Extraction**: Better text cleaning, link validation, metadata extraction\n",
    "4. **Built-in Summarization**: Summarize any scraped website using Ollama\n",
    "5. **Flexible Configuration**: Customize all aspects of scraping and summarization\n",
    "\n",
    "### Key Methods:\n",
    "- `AdvancedWebsite(url)` - Scrape a website\n",
    "- `website.summarize()` - Quick summarization with defaults\n",
    "- `website.summarize_with_ollama()` - Advanced summarization with custom settings\n",
    "- `website.get_contents()` - Get formatted content\n",
    "- `website.get_links()` - Get all links\n",
    "- `website.get_metadata()` - Get page metadata\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
